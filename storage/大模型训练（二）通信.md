# PCIe
PCIe采用树形拓扑结构，PCIe Root Complex上连CPU-Mem,下连PCIe Switch或PCIe Endpoint
- 因此一个host基本上只有一个RC作为上游
由于每个PCIe链路两端只能各连接一个设备，所以需要PCIe Switch来挂载下游更多设备，一个switch只有一个upstream port
# GPU通信
GPU通信需要跨越的层次
```
nvidia-smi topo -m
```
- NV#: 通过NVlink连接 
- PIX：同一PICe Switch
- PXB: 同一PCIe Host bridge 下不同PCIe Switch的互通
- PHB: 同NUMA下通信
- Node: 同NUMA下跨PHB
- SYS: 跨NUMA通信
## GPU Direct
用 PCI Express (PCIe) 的标准特性，在 GPU 和第三方设备之间建立一条点对点 ( Peer-to-Peer) 的直接数据通路。
要求设备共享RC，根据两个设备的挂载情况分为
- 在同一PCIe Switch下：PIX 和 PXB的情况 ，性能最优
- 在CPU的I/O HUB下
- 跨NUMA: 此时数据传输必须跨越 **QPI 或 HyperTransport (HT)** 等 CPU 互联链路

## NVLink
GPU 间直接互联技术 硬件上
- GPU有NVLink 端口

- 物理连接器，用来跨接并连接主板上紧挨着的两个 GPU 显卡。这个桥接器就是实现 NVLink 连接的实体线缆。
![quadro-a6000-bracket-view-2-slot-a6000.svg|200](https://www.nvidia.cn/content/dam/en-zz/Solutions/gtcf20/nvlink/quadro-a6000-bracket-view-2-slot-a6000.svg)

# 通信库
## NCCL
**集合通信库**，用于在多 GPU 和多节点系统上，利用高效的通信算法（如环形和树形）和底层高速互连技术（如 NVLink）
### NCCL Fast Socket
[Google的优化版]([https://github.com/google/nccl-fastsocket](https://github.com/google/nccl-fastsocket))
### MSCCL
[微软](https://github.com/Azure/msccl), 支持业务自定义的集合通讯算法, 相关论文
> GC3: An Optimizing Compiler for GPU Collective Communication
> Synthesizing optimal collective algorithms
> Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads
> TACCL: Guiding Collective Algorithm Synthesis using Communication Sketches

### aws-ofi-nccl
[亚马逊的插件](https://github.com/aws/aws-ofi-nccla)
### 其他
- 阿里的ACCL
- 腾讯的TCCL


