# 为什么需要KV Cache
**自回归** Transformer 模型中，文本是一个 token 一个 token生成的。
> 自回归意思是通过前文的句子生成下一个token, 并添加到句子中，以此类推
- 输入：“今天天气”
	- 输出：“真”
- 输入：“今天天气真”
	- 输出： “好”
在每一步，模型都需要将**所有已生成的 token** 连同新输入的 token 一起送入 Transformer **所有层**的自注意力模块中，**重复计算**前面所有 token 的 K 和 V 矩阵。
## KV 矩阵
自注意力机制依赖三个主要向量（或矩阵，当处理整个序列时）：
1. **Query (Q)**：**查询**。表示当前 token **想要寻找**什么信息。
2. **Key (K)**：**键**。表示序列中每个 token 所**包含的内容/信息摘要**。
3. **Value (V)**：**值**。表示序列中每个 token 所**携带的实际信息**。

输入序列中的每个 token，它会首先经过词嵌入（Embedding）得到一个向量表示。然后，这个向量会通过三个不同的线性变换（矩阵乘法）来生成 Q、K、V 三个向量：
$$ Q = XW_Q $$
$$K = XW_K$$
$$V = XW_V$$
- $X$: 是输入 token 序列的embedding表示
- $W$: 权重矩阵，代表了模型参数
### 自注意力机制
“我喜欢吃苹果，它很甜” -> Transformer
1. 将 **Q** 与序列中的所有 **K** 进行点积计算（矩阵乘法）。得到一组“注意力分数”**（Attention Score），表示 Q 与每个 K 的匹配程度。
	- Transfomer处理到`它` 
		- Q : `它`
		- K: `我， 喜欢， 吃， 苹果`的K向量
	- 将 “它” 的 Q 向量与 “苹果” 的 K 向量匹配，得到一个高分；与 “我” 的 K 向量匹配，得到一个低分。
2. 对注意力分数进行 Softmax 处理。得到注意力权重（Attention Weights）。权重越大，表示该 Token 的信息越重要。
	- 将分数归一化，发现 “苹果” 的注意力权重非常高（比如 90%）。
3. 用**注意力权重**去**加权**所有的 **V** 向量，然后求和。
	- 用这些权重去加权 V 向量。最终生成的上下文向量中，90% 的信息来自于“苹果”的 V 向量。
	- 最后这个加权后向量，随后会被送到下一层或前馈网络，最终用于预测下一个 Token。

## KV Cache
### 为什么
1. 当模型生成第 L+1 个 Token 时，它仍然必须关注**全部 L 个**之前的 Token
2. 由于权重矩阵没变，所以token的kv也不变，因此可以做cache
在第一次生成（或处理 prompt）时，模型计算所有 token 的 K 和 V 矩阵，并将它们存储在内存中（即 KV Cache）。
- 计算复杂度从$L^2$ 变到了L
### 特性
缓存大小随 L 线性增长。**内存占用越来越高。**
$$KV Cache 大小≈2×(层数)×(序列长度 L)×(隐藏维度 dmodel​)×(数据精度)$$
- 2代表：Key和Value都占据大小
- 层数：每层Transformer都有自己的缓存
- 隐藏维度：K 和 V 向量的维度（通常是几千）

# 限制KV Cache大小
## 多注意力头
多查询注意力 (MQA) / 分组查询注意力 (GQA)：让多个注意力头共享同一组 K/V 矩阵，从而直接减少 K 和 V 的数量，显著降低内存需求。
## PagedAttention
借鉴了操作系统中的虚拟内存概念，只为实际需要的 Token 分配内存，而不是预先分配整个批次的最大长度，从而大大减少了内存浪费。

## 限制序列长度
使用**滑动窗口注意力 (Sliding Window Attention)**， 在生成长序列时，只关注最近的 W 个 Token，并丢弃最旧的 K/V 缓存，从而将缓存大小限制在一个固定窗口内。
## 量化
将 K/V 矩阵从 16 位浮点数（FP16）或 32 位浮点数，压缩到 8 位甚至 4 位整数，**减少每个数值的存储空间**。

# 实际负载中的KV Cache
整体上来看，KVCache的设计是跟业务类型高度相关的
- 多轮对话：时间窗口10min以内
- 文档对话：时间窗口更短，复用率更高
>Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention


## KV Cache命中率
> KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache at a Large Cloud Provider

多轮对话Cache复用率更高，文档的Cache命中率更高
1. 用户的Cache的复用率体现出幂律
2. Cache的复用间隔时间体现出幂律（生命周期<10min）
## 多级存储

## KIMI: MoonCake
### 推理过程
1. 预填充：处理完整的输入 Prompt 并计算出第一个输出 Token
	1. 并行性高指的是可以直接对prompt的所有输入token计算
2. 解码：是基于已有的输入和已生成的 Token，自回归地生成后续的 Token，直到回复结束。
	1. 必须线性序列增长
### 特性
| 阶段       | 输入长度 | 计算特点        | 输出目标      |
| -------- | ---- | ----------- | --------- |
| Prefill  | 固定   | 全并行计算，高计算密度 | 首个Token   |
| Decoding | 逐步增长 | 串行生成，高显存带宽  | 后续所有Token |
首Token延迟（TTFT）主要由Prefill阶段决定，而生成间隔延迟（TBT）则由Decoding阶段效率决定）
**资源利用率和负载波动问题（Prefill/Decode 差异）**
- **资源不平衡：** 预填充（Prefill）阶段是**计算密集型**，而解码（Decode）阶段是**内存带宽密集型**。在传统的统一架构中，很难同时高效地利用为这两种不同计算模式设计的资源。
- **负载波动导致资源浪费：** 在高负载场景下，预填充和解码阶段之间的任务时间差（time lag）会导致两者集群的负载出现剧烈的**反相波动（预填充集群的负载高的时候，解码集群的负载低；反之，当预填充集群的负载低的时候，解码集群的负载高）**。这使得集群的资源利用率（GPU利用率）低下，无法充分发挥硬件能力。

**解决方案**
- 两个用不同的GPU资源，并且作为两个不同的阶段，放到不同机器集群，因此可以单独设计资源架构
- 早期拒绝：如果一个请求被接收，它会先进入预填充（计算量大）。但如果在预填充完成后，解码集群已经满载，这个请求的 KVCache 就必须等待，浪费了宝贵的预填充时间。如果调度器根据解码集群什么时候有空闲资源，如果觉得Prefill后没有空闲解码资源，那就直接不Prefill



**长上下文场景下的 KVCache 内存和重复计算问题**
随着 LLM 支持的上下文长度越来越长（例如 128K 或 256K），KVCache 对内存的需求急剧增加：
- **巨大的内存消耗：** KVCache 会占用大量的 GPU 显存，这是部署大型模型和长上下文模型的最大瓶颈之一，限制了每次批处理（Batch Size）可以处理的请求数量，从而降低了整体吞吐量。
- **KVCache 重复计算的浪费：** 许多请求，尤其是聊天场景或分批处理长文档时，可能共享相同的**前缀（Prefix）**。传统方法会为每个请求重新计算这部分前缀的 KVCache，造成大量的重复计算和资源浪费。    
**解决方案**
- 多级缓存
- 前缀复用：调度器会检查它的 Prompt 是否与 KVCache 存储池中已有的任意一个已计算的前缀相匹配
- 解耦架构：一个统一的存储池

**高度过载场景下的效率和延迟保障** (SLO)
对于像 Kimi 这样的 MaaS（模型即服务）提供商，必须在业务爆炸式增长和资源有限的矛盾下，同时保证性能和服务质量：
- **吞吐量与延迟的平衡：** 如何在最大化整体**有效吞吐量（Effective Throughput）的同时，满足严格的**服务水平目标（SLOs）**，特别是对用户体验至关重要的“首个 Token 时间（TTFT）”要求。
- **过载下的资源浪费：** 在系统高度过载时，请求即使被接受并完成了计算量巨大的 **预填充** 阶段，也可能因为没有可用的 **解码** 槽位而被阻塞或最终超时。这种情况下，预填充阶段的计算是完全浪费的。

**解决方案**
- KV缓存
- 早期拒绝：见前面

