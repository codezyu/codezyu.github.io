- 读接口 改为异步

- 内存分配 从bitmap 改成offset
fineMem: Breaking the Allocation Overhead vs. Memory Waste Dilemma in Fine-Grained Disaggregated Memory Management
- 内存维护索引根本就是错误的，根本没有SSD里面有索引的说法，而是在距离计算最近的地方构建索引

	- 如果快速构建索引
	- 缓存：和path的缓存号进行对比 [**[FAST’26]** DMTree: Towards Efficient Tree Indexing on Disaggregated Memory via Compute-side Collaborative Design](https://muouim.github.io/)

- 新工作
	- 共享日志
	- 围绕junanry + scalog
资源非常稀缺，通常受到以下限制（可通过 `ibv_query_device_ex` 查询）：

- **容量限制 (`max_dm_size`)：** 通常仅 **256KB** 到 **1MB**（例如 ConnectX-5/6）。

- NVMe: Octopus

实际上会维护索引/数据存储两个节点
- https://zhuanlan.zhihu.com/p/717793786
	- 分布式缓存： page 

1. 不需要考虑持久性问题：但是会和日志等不相关
	1. HiRadixTree： MoonCake
2. **HiRadixTree**: 只负责本地L1/L2索引，不存储全局信息
3. **Mooncake Store**: 构建集群级别的分布式内存池
4. **Master Service**: 提供全局协调和统一管理
5. **透明访问**: 应用程序可以透明地访问整个集群的内存资源
6. **智能分配**: 自动在集群节点间分配和负载均衡
7. **全局共享**: 任何节点的KV cache都可被集群内所有实例共享
- RPC请求获取页位置：
	- 元数据访问
		- 查询副本位置
	- 数据传输


### . 空间效率与压缩 (Space Efficiency)

内存是昂贵的，索引如果能完全放入内存，性能将是质的飞跃。

- **前缀压缩 (Prefix Compression):** 针对有序 Key（如 B+ Tree 或 LSM Block），只存储 Key 的差异部分。
    
- **Succinct Data Structures (简洁数据结构):** 在接近信息论下限的空间内存储数据，并支持快速查询。
    
    - **应用:** Rank/Select 字典，SuRF (Succinct Range Filter) —— 既是 Bloom Filter 也能支持范围查询。
        
- **Bitmap 索引:** 针对基数（Cardinality）较低的数据列，利用位运算加速查询。

全局屏障 (Distributed Barrier)
在分布式存储系统的语境下，“分布式原语”指的是构建复杂系统所需的**底层逻辑积木**。在 RDMA 时代，这些原语正在经历一场彻底的重构：从**“基于消息传递（Message Passing / RPC）”** 转向 **“基于共享内存（Shared Memory / One-sided）”**。

以下是目前 RDMA 存储系统中最重要的几类分布式原语及其技术细节：

---

### 1. 同步原语 (Synchronization Primitives)

这是所有分布式系统的基石，用于协调不同节点对共享资源的并发访问。

#### **A. 分布式互斥锁 (Distributed Mutex)**

- **传统做法：** 使用 ZooKeeper 或 Redis。客户端发送请求给锁服务，服务排队后返回。
    
- **RDMA 实现（进化路线）：**
    
    1. **CAS 自旋锁 (CAS Spinlock):** 客户端直接对远端的一个 64 位整数进行 RDMA `Compare-and-Swap`。
        
        - _问题：_ 争用剧烈时，大量的 CAS 失败会导致网卡（RNIC）吞吐量暴跌。
            
    2. **RDMA-MCS 锁 (Queue-based Lock):** 将全局争用转化为本地等待。
        
        - _原理：_ 每个想要锁的客户端，在远端内存的“等待队列”末尾挂载自己的节点（RDMA Swap/FAA）。它只需要轮询自己本地内存的一个标志位，等待前一个持有者释放锁时通过 RDMA Write 修改这个标志位。
            
        - _优势：_ 无论多少人抢锁，网络流量都是 $O(1)$ 的。
            
    3. **读写锁 (Reader-Writer Lock):** 针对读多写少场景，允许多个客户端通过 RDMA Read 同时持有读锁（增加引用计数），写锁则独占。
        

#### **B. 全局屏障 (Distributed Barrier)**

- **用途：** 用于机器学习训练（AllReduce）或批量计算，要求所有节点都到达某个同步点后，才能继续执行。
    
- **RDMA 实现：**
    
    - **Dissemination Barrier:** 利用 RDMA Write 实现多轮次的信号传递（类似 Gossip 协议），将延迟从 $O(N)$ 降低到 $O(\log N)$。
        

---

### 2. 内存数据结构原语 (Remote Data Structure Primitives)

如何在不使用 CPU 的情况下，让 RDMA 直接操作复杂的远端数据结构？

#### **A. 远端哈希表 (Remote Hash Table)**

- **挑战：** 哈希冲突如何解决？扩容怎么做？
    
- **RDMA 原语实现：**
    
    - **Hopscotch / Cuckoo Hashing:** 这些算法对 RDMA 友好，因为它们可以通过少量的 RDMA Read 探测位置。
        
    - **Race Hashing (NSDI '19):** 允许并发写入。客户端写入时利用 RDMA CAS 锁定 Bucket。读取时利用 CRC 校验或版本号（Version）来检测读到的数据是否在读取过程中被修改了（Inconsistent Read），如果被改了就重试。
        

#### **B. 远端 SkipList / B+ Tree (Range Index)**

- **用途：** 数据库的范围查询。
    
- **RDMA 原语实现：**
    
    - **乐观并发控制 (Optimistic Concurrency):** 客户端读取 B+ 树节点时，不加锁。读取前后对比节点的“版本号”。如果版本号变了，说明读取期间有人插入了数据，需要重读。
        
    - **HTM (Hardware Transactional Memory) + RDMA:** 服务端利用 CPU 的 HTM 特性保护内存更新，客户端利用 RDMA 读取。
        

---

### 3. 通信与消息原语 (Communication Primitives)

尽管 RDMA 提倡旁路 CPU，但有时候必须通知对方（例如：告诉存储引擎“数据已落盘，请更新元数据”）。

#### **A. 门铃与轮询 (Doorbell & Polling)**

- **RPC 加速：** 传统的 TCP Socket 是基于中断的。RDMA RPC（如 **eRPC**）是基于轮询的。
    
- **实现：**
    
    - **Circular Buffer:** 发送方直接 RDMA Write 数据到接收方的环形缓冲区。
        
    - **rKey 交换:** 握手时交换内存密钥。
        
    - **Last Byte Check:** 接收方 CPU 不断轮询缓冲区槽位的最后一个字节。一旦该字节变为非零，说明整个消息（因为 PCIe 顺序保证）已经到达。这是实现亚微秒级 RPC 的核心原语。
        

#### **B. 共享日志 (Shared Log)**

- **用途：** 它是现代分布式存储（如 CorfuDB, Aurora）的核心抽象。
    
- **RDMA 原语实现：**
    
    - **Global Sequencer:** 一个极小的原子计数器节点，负责分配 Log ID（利用 RDMA FAA - Fetch and Add）。
        
    - **Log Appending:** 客户端拿到 ID 后，直接计算出物理地址偏移，通过 RDMA Write 将日志条目写入多个副本节点的内存中。
        

---

### 4. 事务与一致性原语 (Transaction & Consistency Primitives)

这是数据库领域最难的部分。

#### **A. 远端时间戳 (Remote Timestamping)**

- **用途：** 事务定序（Ordering）。
    
- **RDMA 实现：** 利用 RDMA 原子操作（Atomic Fetch-and-Add）在一个中心化的 Oracle 节点（或者利用原子钟+RDMA同步）获取全局单调递增的时间戳。
    

#### **B. 验证原语 (Validation Primitive)**

- **用途：** 2PC（两阶段提交）的 Prepare 阶段。
    
- **RDMA 实现：** 事务管理器（Coordinator）直接通过 RDMA Read 读取所有参与者（Participants）的锁表或版本号，确认数据未被锁定且版本最新。如果检查通过，直接 RDMA Write 写入 Commit Log。这比传统的询问式 RPC 快得多。
    

#### **C. 租约 (Lease)**

- **用途：** 故障检测与主节点选举。
    
- **RDMA 实现：**
    
    - 主节点必须每隔 X 毫秒对某个特定的远端内存地址执行一次 RDMA Write（续租）。
        
    - 其他节点通过 RDMA Read 检查该地址的时间戳。如果超时未更新，认为主节点死亡，发起新一轮选举。这种方式完全不依赖主节点的 OS 调度，避免了“假死”。
        

---

### 总结：RDMA 原语 vs 传统原语

|**原语类别**|**传统实现 (基于 TCP/RPC)**|**RDMA 原语实现 (基于 Memory)**|**核心优势**|
|---|---|---|---|
|**互斥锁**|向 Lock Server 发送 RPC 请求|本地排队 + RDMA 原子修改队尾指针 (MCS)|消除中心化瓶颈，流量恒定|
|**消息通知**|中断 (Interrupt)|轮询内存末尾字节 (Polling)|延迟从 10us 降至 1-2us|
|**哈希查找**|发送 Key 给服务器，服务器查完返回|客户端计算 Hash -> RDMA Read 探测|服务端 CPU 零负载|
|**日志复制**|Leader 推送 -> Follower 确认|Leader 直接 DMA 写入 Follower 内存|消除 Follower 接收处理延迟|
|**存活检测**|心跳包 (Ping/Pong)|远端内存读写探测 (Keep-alive)|区分 OS 死锁与机器掉电|

### 我可以为您做的下一步

这其中 RDMA-MCS 锁（用于高性能互斥）和 Shared Log（用于数据强一致性）是最具代表性的两个原语。

如果您感兴趣，我可以为您手写一段 C++ 伪代码，展示一个客户端是如何通过 verbs 接口，在没有任何服务端 CPU 参与的情况下，完成一次 “加锁 -> 写入数据 -> 释放锁” 的全过程。


你提到的 CAS（Compare-and-Swap）请求低，具体低到什么程度？原因是什么？

- **现象：**
    
    - RDMA Read/Write 吞吐量可以轻松达到网卡的线速（例如 100M IOPS）。
        
    - 但在**高争用（Contention）**场景下，RDMA CAS/FAA（Fetch-and-Add）的吞吐量会断崖式下跌，通常只有 **2M - 5M IOPS**。哪怕没有争用，单次原子操作的延迟也比 Read 高。
        
- **底层原因：PCIe 协议的限制**
    
    - **总线锁定：** RDMA 网卡执行原子操作时，必须向 PCIe 总线发送 **Read-Modify-Write** 的 TLP (Transaction Layer Packet) 请求。
        
    - **全局序列化：** 为了保证原子性，目标机器的 PCIe 控制器（Root Complex）必须对该内存地址进行**锁定**，直到操作完成。
        
    - **内部调度：** 现在的网卡（如 ConnectX 系列）内部处理 Atomic 请求的引擎数量有限。当成千上万个客户端同时 CAS 同一个热点地址（Hotspot）时，网卡内部队列会堵死，导致严重的排队延迟。
        
- **影响：** 这使得直接用 RDMA CAS 实现分布式锁（如 Spinlock）在规模化后变得不可用，迫使研究者转向 MCS 队列锁（如 _Fast RDMA-based Ordered Locks, OSDI '18_）或 _ShiftLock (FAST '25)_ 这种避免 CAS 的设计。


数据分布
- 读写冲突频繁的
- 