Header:

SIGMOD '22. 本文研究了在数据量超出单节点甚至集群总内存容量时，如何利用 RDMA 和 NVMe SSD 打破“内存墙”与“存储墙”之间的性能鸿沟，指出当前硬件的主要物理限制在于 NVMe SSD 的高延迟（相比 DRAM 高 2 个数量级）以及 RDMA 原子操作（Atomics）在分布式环境下的性能瓶颈与 CPU 原子操作的不兼容性。

# 问题

- 内存数据库的成本与扩展性悖论
    
    纯内存数据库（In-memory DBMS）要求所有数据驻留内存。随着数据集增长，DRAM 价格并未线性下降，导致硬件成本爆炸。即便是分布式内存系统（如 FaRM, NAM-DB），虽然聚合了多机内存，但一旦工作集（Working Set）超出集群总内存，系统缺乏高效的机制将冷数据卸载到 SSD，导致无法处理大于内存的数据集 111111111。
    
- NVMe 存储引擎的延迟断崖 (Latency Cliff)
    
    现有的单节点 NVMe 存储引擎（如 LeanStore）虽然利用了 SSD，但在工作集超出内存时，性能会急剧下降。这是因为 NVMe SSD 的延迟（约 $78 \mu s$）仍比 DRAM（约 $0.1 \mu s$）高出至少两个数量级 2222。这种延迟差异对 OLTP 等延迟敏感型负载是致命的。
    
- RDMA 原语的底层开销与不兼容性
    
    现有的分布式共享内存系统（如 NAM-DB）依赖 RDMA 单边原子操作（Atomic Fetch-and-Add）来管理分布式锁。然而，作者指出：
    
    1. **性能差**：网络原子操作比 CPU 原子操作慢得多，且会影响其他非原子 RDMA 操作的性能 3。
        
    2. **不一致性**：RDMA 原子操作与本地 CPU 原子操作在硬件上是不兼容（incompatible）的 4。这意味着本地访问也必须强制使用昂贵的 RDMA 原子操作来保证一致性，导致本地访问延迟增加至 $1 \mu s$ 级别 5。
        

> Researcher's Comment
> 
> 这是一个非常敏锐的硬件洞察。很多人误以为 RDMA 就是银弹，可以直接 bypass CPU 做所有事。但作者点出了一个关键的 Trade-off：为了支持 RDMA 原子性而牺牲本地 CPU 访问的极速性能是不划算的。ScaleStore 放弃依赖 RDMA Atomics，转而在软件协议层面上设计复杂的 RPC 机制，这种“退一步”的设计反而换来了更高的整体吞吐量，这在异构硬件设计中体现了很好的 First Principles 思维。

# 解决方案 (The Solutions)

## 基于页粒度的分布式缓存一致性协议

ScaleStore 抛弃了传统 CPU 缓存一致性协议（如 MESI）中以 Cache Line（64 Bytes）为粒度的做法，改为以 **页（Page, 4KB）** 为单位 6。

- **状态机设计**：引入了 `Node-Exclusive`（节点独占，用于写）和 `Node-Shared`（节点共享，用于读）两种所有权模式 7。
    
- **目录节点（Directory Node）解耦**：每个页面通过 PID 映射到一个固定的目录节点。目录节点维护页面的元数据（位置、所有权状态），但**不一定**持有最新的页面数据 8888。
    
- **本地热路径优化**：工作线程首先检查本地转换表（Translation Table）。如果拥有正确的所有权，则直接在本地 DRAM 访问，无需任何网络开销。这使得热点数据的访问速度等同于单机内存数据库 9999。
    

<img src="Figure 2: Protocol Flow" alt="image" width=100% />

> Researcher's Comment
> 
> 这里的核心权衡在于“粒度”。CPU 的 MESI 协议为了低延迟必须用细粒度，但在分布式 + SSD 场景下，网络和 I/O 的延迟使得 4KB 页面成为更经济的单位。这种设计由硬件特性倒推软件架构，不仅均摊了 RDMA 和 SSD 的访问开销，还简化了元数据管理的复杂度。

## 预期链式转发 (Anticipatory Chaining)

为了解决多个节点争抢同一页面时的冲突和公平性问题，作者提出了一种无需忙轮询（Busy Polling）且优于传统 FIFO 队列的机制。

- **立即元数据更新 (Immediate Metadata Updates)**：当请求到达目录节点时，目录**立即**更新元数据，将该页面的“逻辑所有者”指向当前的请求者，即使页面物理上还没有传输 10。
    
- **逻辑前驱转发**：后续的请求者会被目录节点告知去联系“逻辑前驱”（即上一个请求者）。这样，请求者们形成了一条链，页面将沿着这条链按序传输 11。
    
- **所有者稳定性 (Owner Stability)**：引入“冲突纪元”（Conflict Epoch）。当目录发现冲突时增加纪元号。如果逻辑前驱试图驱逐该页面，会因为纪元号不匹配而被拒绝，从而保证页面确实会存在于逻辑前驱处，直到传输完成 12121212。
    

<img src="Figure 5: Handling multiple conflicts" alt="image" width=100% />

> Researcher's Comment
> 
> 这个设计非常精妙，它本质上是一种分布式下的“票据锁”（Ticket Lock）变体。通过让目录节点只负责“发号”（更新逻辑所有者），而不负责“传球”（数据传输），消除了目录节点的带宽瓶颈。同时，利用逻辑链条避免了请求者反复重试（Busy Retry）带来的网络风暴，极其优雅地解决了分布式争用下的尾延迟问题。

## 基于纪元的异步驱逐与 NVMe 优化

为了处理极高的 RDMA 吞吐量，ScaleStore 需要高效的冷热数据识别与驱逐机制。

- **Epoch-based LRU 近似**：不维护全局 LRU 链表（避免锁竞争）。使用全局纪元计数器，每次访问页面时仅更新页面帧上的 `lastEpoch` 字段 13。
    
- **采样驱逐**：后台线程（Page Provider）随机采样页面，根据纪元阈值识别冷页面进行驱逐 14。
    
- **RDMA 与 NVMe 协同**：
    
    - 利用 RDMA 单边写（One-sided Write）进行页面传输，因为它可以异步执行且不需要接收端 CPU 参与 15。
        
    - 在驱逐脏页到 SSD 时，使用 `libaio` 和 `O_DIRECT` 绕过操作系统缓存，直接跑满 NVMe 带宽 16。
        

# 核心亮点 (Key Insights & Trade-offs)

- 透明的内存抽象 (Transparent Memory Abstraction)
    
    ScaleStore 成功地将集群内所有节点的 DRAM 和 NVMe 抽象为一个统一的全局地址空间。对于上层应用（如 B-Tree），无论数据是在本地内存、远程内存还是远程 SSD，都可以通过统一的 Guard 接口访问 17。这种抽象屏蔽了底层复杂的分布式一致性和存储层级。
    
- 用空间换时间的一致性策略
    
    在处理 Shared Conflict（读写冲突）时，ScaleStore 选择发送失效消息给所有共享者，这看起来开销很大。但作者利用 RDMA 的低延迟特性并行发送消息，并指出在实际负载中，一个页面的共享者通常很少 18。这是一种典型的利用现代网络高带宽特性来简化协议状态机的设计哲学。
    
- 极致的 RDMA 优化
    
    系统并未盲目使用 RDMA 单边原语。对于控制流（RPC），使用了基于 Cache Line 大小的消息邮箱（Mailbox）机制 19；对于数据流（页面传输），使用了单边写；而对于原子性，则完全在软件层通过目录节点协调。这种针对不同操作特征选择不同 RDMA原语（Verbs）的策略，最大化了硬件效能。