问题：MoE的DeepSeek 需要在不同计算节点之间进行大量的“all-to-all”通信，这导致计算与通信的效率比约为 1:1
# 流水线
## 1F1B
之前的流水线并行是在模型分层计算的流水线，在GPU间链式流转，以完成前向传播
1. 启动气泡：后面的 GPU 都在等待前面的 GPU 完成工作，因此有很多 GPU 处于空闲状态
2. 稳定运行：所有GPU到达高负荷
3. 排空气泡：结束阶段，反向传播中，前面的GPU空闲
### 问题
**启动和结束阶段的“气泡”非常大**。根据公式$(PP-1)(F+B)$，这个“气泡”的大小与流水线的深度（GPU数量）成正比 。流水线越长（并行训练用的GPU越多），浪费的空闲时间就越多，整体效率就越低
> **PP**: 流水线并行（Pipeline Parallelism）的阶段数 跟GPU数量有关
> **F**: 指的是单个GPU处理一个批次（chunk）数据所需的前向传播（Forward）执行时间
> **B**: 指的是单个GPU处理一个批次数据所需的完整反向传播（Backward）执行时间

## ZB1P
反向传播中，可以分为两个任务
- 本机权重的梯度，用于更新自己的参数，因此可以推迟执行
- 为输入计算梯度：由于链式法则（即为前一个GPU计算前一个GPU输出相对损失的梯度， 保证梯度向前传播），所以前一个GPU在等，这个优先做，减少等待时间
所以气泡就是$(PP-1)(F+B-2W)$
>**W**: 是计算本机权重梯度的时间，由于启动和排空都有，所以-2W

## DualPipe
- **双向流水线调度**，即同时从流水线的头和尾输入数据
- 它不仅推迟 `Wgrad`，还将整个前向传播（F）和反向传播（B）的任务打得更碎（如 Attention、MLP、Dispatch 等），然后像“华容道”一样**重新穿插排列**
